---
title: "Predicting Brand"
author: "Kais Kawar"
date: "11 April 2019"
output: html_document
---

```{r include=FALSE}
library(ggplot2, quietly = T)
library(caret, quietly = T)
library(readr, quietly = T)
library(C50, quietly = T)
library(mlbench, quietly = T)
library(bda, quietly = T)
library(zoo, quietly = T)
library(pander, quietly = T)
library(kableExtra, quietly = T) 
```

## Task Overview

The sales team engaged a market research firm to conduct a survey of our existing customers. One of the objectives of the survey was to find out which of two brands of computers our customers prefer. This information will help us decide with which manufacturer we should pursue a deeper strategic relationship. Unfortunately, the answer to the brand preference question was not properly captured for all of the respondents.

That is where you come in: I want you to investigate if customer responses to some survey questions (e.g. income, age, etc.) enable us to predict the answer to the brand preference question. If we can do this with confidence, I would like you to make those predictions and provide the sales team with a complete view of what brand our customers prefer.

To do this, I would like you to run and optimize at least two different decision tree classification methods in R - C5.0 and RandomForest - and compare which one works better for this data set. 

I have already set up the data for you in the attached CSV files: the file labelled CompleteResponses.csv is the data set you will use to train your model and build your predictive model. It includes 10,000 fully-answered surveys and the key to the survey can be found in survey_key.csv. The file labelled SurveyIncomplete.csv will be your main test set (the data you will apply your optimized model to predict the brand preference). You'll be applying your trained and tested model to this data to prepare the model for production.

###Loading and preprocessing the data
Summary shows us that the data has no outliers or missing values
```{r}
completeResp <- read.csv("data/CompleteResponses.csv")
summary(completeResp)
```

###Importance of each feature used
In order to choose the features in the model, a correlation matrix was created in order to see the relationship of all the features with one another. This was done before discretising 4 of the features as the correlation matrix only works on numerical features.
We got the outcome in the figure below. The only correlation in the data is a weak one between salary and brand (0.2), that’s why our focus was on salary to predict the brand.

```{r}
cor(completeResp)
```
We can see that only salary has a some positive correlation with the brand, so our focus will be on this feature.

After reading the surveykey.xlsx file, we know that some of our features should be converted to Factor and shouldn't be numerical.

```{r}
str(completeResp)

#Preprocessing the data
completeResp[,c(3:5,7)] <- lapply(completeResp[,c(3:5,7)] , factor)
```

Now we need to split the data into training and testing sets to create and assess our models
```{r}
#Set seed to know the random order
set.seed(998)

#define an 75%/25% train/test split of the dataset
inTraining <- createDataPartition(completeResp$brand, p = .75, list = FALSE)
training <- completeResp[inTraining,]
testing <- completeResp[-inTraining,]
```


###Classifiers Tried: {.tabset .tabset-fade}
####C5.0
C5.0 can produce a classifier expressed either as a decision trees or rulesets. In many applications, rulesets are preferred because they are simpler and easier to understand than decision trees.

```{r}
#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)

#Set seed to know the random order
set.seed(998)

#train Linear Regression model
# The tuneGrid parameter lets us decide which values the main parameter will take,
# while tuneLength only limit the number of default parameters to use.
C5Fit <- train(brand~., data = training, method = "C5.0", trControl=fitControl, tuneLength = 2)

#check the results
C5Fit

#How the model prioritized each feature in the training
varImp(C5Fit)
```

####Random Forests 
Random Forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. It is good against overfitting due to the randomness of the algorithm.

```{r}

#10 fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)

#Dataframe for manual tuning of mtry
rfGrid <- expand.grid(mtry=c(3,6,10,12,18))

#Set seed to know the random order
set.seed(998)

#Train Random Forest Regression model
rfFit <- train(brand~., data = training, method = "rf", trControl=fitControl, tuneGrid=rfGrid)

#training results
rfFit

#How the model prioritized each feature in the training
varImp(rfFit)
```

####Gradient Boosting 
Gradient Boosting is a machine learning technique which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.

```{r}

#10 fold cross validation repeated 10 times
fitControlGBM <- trainControl(method = "repeatedcv", number = 10, repeats = 1)

#Grid to define our own parameters for this classifier
gbmGrid <-  expand.grid(interaction.depth = c(1,5,9), 
                        n.trees = (10)*50, 
                        shrinkage = c(0.1),
                        n.minobsinnode = 20)

#Set seed to know the random order
set.seed(998)

#Train GBM Regression model
gbmFit <- train(brand~., data = training, 
                 method = "gbm", 
                 trControl = fitControlGBM, 
                 verbose = FALSE, 
                 ## specifying the exact models 
                 ## to evaluate:
                 tuneGrid = gbmGrid)

#training results
gbmFit

#How the model prioritized each feature in the training
#varImp(gbmFit)
```

###

###Classifier selected to make the predictions
Below are the best metrics for each classifier I used for this task:

```{r echo = FALSE, results = 'asis'}
Classifier <- c('Random Forest','C5.0','Gradient Boosting Trees')
Accuracy <- c(0.923, 0.923, 0.926)
Kappa <- c(0.837, 0.836, 0.844)
metrics <- data.frame(Classifier, Accuracy, Kappa)
kable(metrics)
```

I picked the most accurate model after optimization which was the gradient boosting classifier. I have very high level of confidence because after testing 3 algorithms and getting similar accuracy, we are safe from over-fitting (this will be rectified with postResample() method).

###Model Accuracy
Now that we have our model, we will apply it to the test set and see how accurate it can predict

```{r}
pred <- predict(gbmFit, newdata = testing)
summary(pred)
postResample(pred, testing$brand)
```

After using postResample() with the predictions and the test set, we got accuracy and kappa that are almost exact to the results we achieved during model optimization process.

This is good news as it means our model is good and isn’t overly-fitted. And since our data has no outliers, it’s safe to use this model to predict the missing values in our survey,

###Loading and preprocessing incomplete data
```{r}
incompleteData <- read.csv("data/SurveyIncomplete.csv")

#preprocess
incompleteData[,c(3:5,7)] <- lapply(incompleteData[,c(3:5,7)] , factor)

```

###Predicting incomplete data
Finally, now we can predict the incomplete questions in the survey using the most optimized model.
```{r}
#predict incomplete data
incompleteData$brand <- predict(gbmFit, newdata = incompleteData)
summary(incompleteData$brand)
```

As seen in the summary, the model predicts that customers would pick sony 61.5% of the time over acer

###Charts showing predicted answers of the incomplete data and the overall data
```{r}
#combining both data frames
completeResp <-do.call(rbind, list(incompleteData, completeResp))
summary(completeResp)

#change 0 to online and 1 to store (more meaningful)
completeResp$brand <- ifelse(completeResp$brand == 1, "Sony", "Acer")

#using ggplot to draw barchart for incomplete data
ggplot(data = incompleteData, mapping = aes(x = brand, fill = brand)) +
  geom_bar(position = "dodge") +
  labs(x="Brand", y="Count", title="Brand Predictions for the incomplete data") +
  geom_text(mapping = aes(label = sprintf("%0.2f",..count../sum(..count..)*100)), stat = "count", vjust = -0.5)+
  expand_limits(y=3400)
label=sprintf("%0.2f", round(a, digits = 2))

#using ggplot to draw barchart for complete data
ggplot(data = complete, mapping = aes(x = brand, fill = brand)) +
  geom_bar(position = "dodge") +
  labs(x="Brand", y="Count", title="Brand Predictions for the complete data") +
  geom_text(mapping = aes(label = sprintf("%0.2f",..count../sum(..count..)*100)), stat = "count", vjust = -0.5)+
  expand_limits(y=10000)
label=sprintf("%0.2f", round(a, digits = 2))
```
